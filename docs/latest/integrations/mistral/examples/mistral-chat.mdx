---
title: 'Chat'
description: 'Monitor Mistral `chat` API usage for cost, performance & user interactions'
---

<Tabs>
  <Tab title="Python">

  ### Synchronous Usage
  <CodeGroup>
    ```python default
    from mistralai.client import MistralClient
    import dokumetry

    mistral = MistralClient(
      api_key="YOUR_MISTRAL_API_KEY"
    )

    # Pass the above `mistral` object along with your Doku Ingester URL and API key and this will make sure that all Anthropic calls are automatically tracked.
    dokumetry.init(llm=mistral, doku_url="YOUR_DOKU_INGESTER_URL", api_key="YOUR_DOKU_TOKEN")

    response = mistral.chat(
      model="mistral-small-latest",
      messages=[ChatMessage(role="user", content="Guide to Monitor my LLM Applications")],
    )
    print(response)
    ```

    ```python streaming
    from mistralai.client import MistralClient
    import dokumetry

    mistral = MistralClient(
      api_key="YOUR_MISTRAL_API_KEY"
    )

    # Pass the above `mistral` object along with your Doku Ingester URL and API key and this will make sure that all Anthropic calls are automatically tracked.
    dokumetry.init(llm=mistral, doku_url="YOUR_DOKU_INGESTER_URL", api_key="YOUR_DOKU_TOKEN")

    response = mistral.chat_stream(
      model="mistral-small-latest",
      messages=[ChatMessage(role="user", content="Guide to Monitor my LLM Applications")],
    )
    for chunk in response:
      print(chunk)
    ```
    </CodeGroup>
    
    ### Asynchronous usage
    <CodeGroup>
    ```python default
    import asyncio
    from mistralai.client import AsyncMistralClient
    import dokumetry

    mistral = AsyncMistralClient(
      api_key="YOUR_MISTRAL_API_KEY"
    )

    # Pass the above `mistral` object along with your Doku Ingester URL and API key and this will make sure that all Anthropic calls are automatically tracked.
    dokumetry.init(llm=mistral, doku_url="YOUR_DOKU_INGESTER_URL", api_key="YOUR_DOKU_TOKEN")

    async def main() -> None:
      response = mistral.chat(
        model="mistral-small-latest",
        messages=[ChatMessage(role="user", content="Guide to Monitor my LLM Applications")],
      )
      print(message)

    asyncio.run(main())
    ```

    ```python streaming
    import asyncio
    from mistralai.client import AsyncMistralClient
    import dokumetry

    mistral = AsyncMistralClient(
      api_key="YOUR_MISTRAL_API_KEY"
    )

    # Pass the above `mistral` object along with your Doku Ingester URL and API key and this will make sure that all Anthropic calls are automatically tracked.
    dokumetry.init(llm=mistral, doku_url="YOUR_DOKU_INGESTER_URL", api_key="YOUR_DOKU_TOKEN")

    async def main() -> None:
      response = mistral.chat_stream(
        model="mistral-small-latest",
        messages=[ChatMessage(role="user", content="Guide to Monitor my LLM Applications")],
      )
      for chunk in response:
        print(chunk)
    
    asyncio.run(main())
    ```
    </CodeGroup>
  </Tab>
  <Tab title="Javascript">
    <CodeGroup>
    
    ```javascript default
    import MistralClient from '@mistralai/mistralai';
    import DokuMetry from 'dokumetry';

    const mistral = new MistralClient("YOUR_MISTRAL_API_KEY");

    // Pass the above `mistral` object along with your Doku Ingester URL and API key and this will make sure that all Anthropic calls are automatically tracked.
    DokuMetry.init({llm: mistral, dokuUrl: "YOUR_DOKU_INGESTER_URL", apiKey: "YOUR_DOKU_TOKEN"})

    async function main() {
      const response = await mistral.chat({
        model: 'mistral-small-latest',
        messages: [{role: 'user', content: 'What is LLM Observability?'}],
      });

      console.log(response);
    }

    main();
    ```

    ```javascript streaming
    import MistralClient from '@mistralai/mistralai';
    import DokuMetry from 'dokumetry';

    const mistral = new MistralClient("YOUR_MISTRAL_API_KEY");

    // Pass the above `mistral` object along with your Doku Ingester URL and API key and this will make sure that all Anthropic calls are automatically tracked.
    DokuMetry.init({llm: mistral, dokuUrl: "YOUR_DOKU_INGESTER_URL", apiKey: "YOUR_DOKU_TOKEN"})

    const response = await mistral.chatStream({
      model: 'mistral-small-latest',
      messages: [{role: 'user', content: 'What is LLM Observability?'}],
    });

    for await (const message of response) {
      console.log(message)
    }
    ```
    </CodeGroup>
  </Tab>
</Tabs>

